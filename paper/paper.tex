%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{booktabs} % for professional tables
\usepackage[latin1]{inputenc}
\usepackage{tikz}

\usetikzlibrary{shapes,automata,arrows,positioning,calc}

% Define block styles

\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
text width=5em, text centered, rounded corners, minimum height=2em, node distance=2cm]
\tikzstyle{block8} = [rectangle, draw, fill=blue!7, 
text width=5em, text centered, rounded corners, minimum height=2em, node distance=2cm, minimum width=8cm]
\tikzstyle{block10} = [rectangle, draw, fill=blue!7, 
text width=5em, text centered, rounded corners, minimum height=2em, node distance=2cm, minimum width=10cm]
\tikzstyle{bigblock10} = [rectangle, draw, fill=red!20, 
text width=5em, text centered, rounded corners, minimum height=4em, minimum width=10cm]
\tikzstyle{bigblock14} = [rectangle, draw, fill=red!20, 
text width=5em, text centered, rounded corners, minimum height=4em, minimum width=14cm]
\tikzstyle{line} = [draw, -latex']


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}
\usepackage{mathtools}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2019}
% \usepackage[style=authoryear]{biblatex}
% \addbibresource{example_paper.bib}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2019}

\begin{document}

\twocolumn[
\icmltitle{Extending BERT for multi-choice problems \\}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Orr Dermer}{equal,cs}
\icmlauthor{Afek Adler}{equal,in}
\end{icmlauthorlist}

\icmlaffiliation{in}{Department of Industrial Engineering, University of Tel Aviv, Tel Aviv, Israel}
\icmlaffiliation{cs}{Department of Computer Science, University of Tel Aviv, Tel Aviv, Israel.}


\icmlcorrespondingauthor{Afek Adler}{afekilayadler@gmail.com}
\icmlcorrespondingauthor{Orr Dermer}{odermer@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, NLP}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Deep learning models are still far away from human-level performance in commonsense reasoning tasks.
We introduce a multi-choice question answering scheme that models the dependencies between answers by
building a relationship graph between the answers for a given question. 
Then, we compute a centrality measure for each node (answer) in the graph and predict the answer by taking the node with the maximum value of that centrality measure.
We test ourselves on the challenging CommonsenseQA dataset and show that although our method is capable of modeling more complex relationships between the answers we did not manage to improve the current state of the art on this task.
\end{abstract}

\section{Introduction}
When we face a multi-choice question answering (QA) problem where we have to choose the most probable answer, we arrive at it with prior information and knowledge. for instance, we have some kind of a

% TODO: Complete the previous sentence??

While in some cases the state of the art machine-learning methods which were trained on massive amount of data to infer prediction roles
outperforms human-level by a large margin (e.g. SQUAD 2.0 \cite{rajpurkar2018know}),
In the field of common sense reasoning  \cite{talmor2018commonsenseqa} state of the art algorithms fail to generalize and are yet inferior to human level performance.
This is due to two main reasons: first, is that when using algorithms such as fine-tuning a pre-trained language model, it is likely that this knowledge (common sense) does not exist explicitly in the pre training corpus, and second, is due to the fact that creating massive datasets for this purpose solely is hard, and developed only in the last years mostly by crowdsourcing.

Due to this inherent difficulty of this task, the research community invests resources in creating larger-scale datasets with different difficulty levels, e.g., the SWAG \cite{zellers2018swag} dataset with ~100K examples for which \cite{devlin2018bert} demonstrate a model which is already at a human level, and the CommonseneQA \cite{talmor2018commonsenseqa}, a dataset which was made especially for common sense reasoning with \~{}10K samples where the state of the art is much lower then human-level. For a detailed survey of resources see \cite{storks2019commonsense}. 

In this work, we suggest and evaluate models which eliminate the Independence assumption between answers for a given question, aiming to improve the benchmark on the CommonsenseQA dataset. 

\section{Background and Related Work}
Research on machine common sense QA has long been acknowledged for its critical component in natural language understanding. However, only recently, with the embracing of deep learning in natural language processing and crowdsourcing to create larger scale datasets, major breakthroughs has emerged; To the best of the author's knowledge, the first large scale common sense reasoning QA dataset was introduced is 2018 \cite{storks2019commonsense}.

Due to the fact that the prominent approach to solve common sense QA problems is with word embeddings (WE) and contextualized word embeddings (CWE) in the next section we will provide a brief introduction to CWE and explain how to use it in the context of common sense QA.

% We would like to note that this paper address specifically solutions for multi-choice QA with common sense reasoning and not closely related themes such as visual commonsense reasoning or co reference resolution.

\subsection{Contextualized Word Embeddings}
Based on the pioneering work of Word2Vec \cite{mikolov2013distributed} which enabled words to be represented as vectors, thus
enriching the information contained in the input for a variation of language based machine learning tasks, many models extended the Word2Vec concept by taking context into account; the same word has a different meaning in different context. The leading approaches to obtain a contextualized word representations is using the hidden layer of an LSTM \cite{hochreiter1997long} and more specifically a Bi-LSTM such as in ELMO \cite{peters2018deep} or more recently using a transformer architecture such as BERT \cite{devlin2018bert,yang2019xlnet}.

The great power of WE or CWE is to enable using a generic method as a basis for task specific machine learning models; For instance, in the field of natural language processing there are many different tasks (e.g. machine translation, sentiment analysis), yet a simple model based on CWE with fine tuning on a given dataset usually provides much better results than previous models on a given task, creating a new benchmark for this specific task. Companies interested in the field of natural language processing often release those trained models as an open source service, which makes CWE easy to use for academics and in the industry as one.

CWE is a type of a transfer learning method; a model is pretrained on a base dataset and fine tuned on the specific task dataset,  with the underlying assumption that information distillation is possible from one source (base dataset) to the other (task specific dataset).
The main reason for the great success of CWE is that the base model is trained using supervised learning on huge amounts of data, e.g. BERT, was trained on 3.3B words. The fact that CWE models are also self trained makes it easier to collect such huge amount of data without the need to label samples. The downside of these models is that they use many parameters (BERT Large - 340M) - so much such that training and infering requires large processing power and latency is a challenge for creating real time systems. In the next section we will introduce the benchmark model which we aim to improve - BERT for CommonsenseQA (which is identical to BERT for SWAG) as was introduced in the original paper of BERT.

\subsection{BERT}
BERT is a word based CWE model that was pre-trained on two learning objectives: Masked word predictions (15\% of the words were masked) and next sentence predictions. BERT's model  architecture is a multi-layer bidirectional Transformer encoder \cite{vaswani2017attention} with the following hyper-parameters (BERT Large) - 12 Transformer blocks, hidden size 1024 (H) and 16 self attention heads. BERT takes as input one or two sentences.

BERT's input representation (figure \ref{bert_input_repr}) uses WordPeace \cite{wu2016google} tokenization which is a data driven method that aims to achieve a balance between vocabulary size and out-of-vocab words. This way BERT stores only 30K vocabulary words, yet very rarely encounters out-of-vocab words when tokenizing English texts. Two special token are added for each sentence, a start token [CLS] and an end token [SEP]. If the input contains two sentences another [SEP] token is added in between the sentences. Due to the fact that Transformers do not encode the sequential nature of their inputs a segment ids and position embeddings are added as well to the input.

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{bert_input_repr}}
\caption{input representation for BERT (from the original paper).}
\label{bert_input_repr}
\end{center}
\vskip -0.2in
\end{figure}

Fine tuning BERT is performed by appending the task specific classification layer with a given loss function to the pretrained model, training using the specific task's input / output, and changing parameters using back-propagation.

\subsection{BERT baseline for SWAG}

In the BERT Paper, the authors approach the SWAG challange with the described input (figure \ref{fig:model_orig}). that input is fed to BERT's architecture and the [CLS] token embedding is used to predict a logit by calculating a dot product with a learned vector. this process is done for each answer independently of each other and then a softmax layer with cross entropy loss is calculated to compute the error. As we can see, this architecture introduces only H (1024) new parameters to the existing BERT model.

\begin{figure}[h!]
	\begin{center}
		\scalebox{0.7}{
			\begin{tikzpicture}[node distance = 2cm, auto]
			\node [bigblock10] (bert) at (0,0) {Pretrained BERT };
			\node [block, below of=bert] (SEP1) {[SEP]};
			\node [block, left of=SEP1] (question) {Question};
			\node [block, left of=question] (CLS) {[CLS]};
			\node [block, right of=SEP1] (answer) {Answer};
			\node [block, right of=answer] (SEP2) {[SEP]};
			\node [block] (classifier) at (-4,2) {Classifier token};
			\node [block8] (rest) at (1,2) {Unused tokens};
			\path [line] (CLS) -- ([xshift=-4cm]bert.south);
			\path [line] (question) -- ([xshift=-2cm]bert.south);
			\path [line] (SEP1) -- (bert.south);
			\path [line] (answer) -- ([xshift=2cm]bert.south);
			\path [line] (SEP2) -- ([xshift=4cm]bert.south);
			\path [line] ([xshift=-4cm]bert.north) -- (classifier.south);
			
			\path [line,dashed] ([xshift=-2cm]bert.north) -- ([xshift=-3cm]rest.south);
			\path [line,dashed] ([xshift=-0cm]bert.north) -- ([xshift=-1cm]rest.south);
			\path [line,dashed] ([xshift=2cm]bert.north) -- ([xshift=1cm]rest.south);
			\path [line,dashed] ([xshift=4cm]bert.north) -- ([xshift=3cm]rest.south);
			
			\end{tikzpicture}
		}
	\end{center}
	\caption{The original paper's question-answering model.}
	\label{fig:model_orig}
\end{figure}

% \begin{equation}
% [CLS][Question][SEP][Answer][SEP] 
% \label{original_input_order}
% \end{equation} 

\section{CommonSeneseQA}
CommonSenseQA is a dataset specifically made for common sense reasoning, by asking crowd workers to generate questions from concepts from ConceptNet. It contains 12,247 examples and 5 answers per question. 
the Authors use the same architecture as in the BERT benchmark for SWAG and report an accuracy score of 56.7\%. The current state of the art on this dataset is currently 68.3\% accuracy, which is 23.6\% below human level.

\section{Methodology}
We propose a method with different variants to model the dependencies between answers, by feeding answer pairs jointly to BERT's architecture. Doing so, we contribute by:

\begin{itemize}
\item making the model more expressive by removing the independence assumption
\item extending the model to deal with questions as 'what is largest?"
\end{itemize}

We derive our model from the BERT baseline for SWAG. However, as we want our model to compare between the different possible answers, we deviate from the original model by introducing the model with the question and 2 possible answers as each input sequence, for which the model returns a score - 1 meaning the first answer is more likely, and 0 if the second answer is the likeliest. Having received a score for each of pairs, we then integrate those into probability scores for each of the original answers - and then choose the one with the highest probability as our predicted answer. A full overview of our model can be seen in figure \ref{fig:full_model_overview}.

\tikzset{
	font={\fontsize{15pt}{14}\selectfont}}

\begin{figure}[h]
	\begin{center}
		\scalebox{0.5}{
			\begin{tikzpicture}[node distance = 2cm, auto]
			\node (a1) at (0,0) {Answer 1};
			\node (a2) at (3,0) {Answer 2};
			\node (a3) at (6,0) {Answer 3};
			\node (a4) at (9,0) {Answer 4};
			\node (a5) at (12,0) {Answer 5};
			
			\node [block] (bert1) at (0,3) {BERT based pair score model};
			\node [block] (bert2) at (2,3) {BERT based pair score model};
			\node [block] (bert3) at (4,3) {BERT based pair score model};
			\node (threedots) at (6,3) {...};
			\node [block] (bert4) at (8,3) {BERT based pair score model};
			\node [block] (bert5) at (10,3) {BERT based pair score model};
			\node [block] (bert6) at (12,3) {BERT based pair score model};
			
			\path [line] (a1) -- (bert1.south);
			\path [line] (a2) -- ([xshift=0cm]bert1.south);
			\path [line] (a1) -- (bert2.south);
			\path [line] (a3) -- ([xshift=0cm]bert2.south);
			\path [line] (a1) -- (bert3.south);
			\path [line] (a4) -- ([xshift=0cm]bert3.south);
			\path [line] (a2) -- (bert4.south);
			\path [line] (a5) -- ([xshift=0cm]bert4.south);
			\path [line] (a3) -- (bert5.south);
			\path [line] (a5) -- ([xshift=0cm]bert5.south);
			\path [line] (a4) -- (bert6.south);
			\path [line] (a5) -- ([xshift=0cm]bert6.south);
			
			\node [bigblock14, fill=yellow!20,text width=10cm] (integration) at (6,8) {Pair score integration};
			
			\path [line] (bert1.north) -- node [text width=1.3cm, align=center]{Score for (1,2) pair} ([xshift=-6cm]integration.south);
			\path [line] (bert2.north) -- node [text width=1.3cm, align=center]{Score for (1,3) pair} ([xshift=-4cm]integration.south);
			\path [line] (bert3.north) -- node [text width=1.3cm, align=center]{Score for (1,4) pair} ([xshift=-2cm]integration.south);
			\path [line] (bert4.north) -- node [text width=1.3cm, align=center, xshift=1.7cm]{Score for (2,5) pair} ([xshift=2cm]integration.south);
			\path [line] (bert5.north) -- node [text width=1.3cm, align=center, xshift=1.7cm]{Score for (3,5) pair} ([xshift=4cm]integration.south);
			\path [line] (bert6.north) -- node [text width=1.3cm, align=center, xshift=1.7cm]{Score for (4,5) pair} ([xshift=6cm]integration.south);
			
			\path [line,dashed] ([yshift=0.5cm]threedots.north) -- (integration.south);
			
			\node [text width=2cm](s1) at (0,11) {Score for answer 1};
			\node [text width=2cm](s2) at (3,11) {Score for answer 2};
			\node [text width=2cm](s3) at (6,11) {Score for answer 3};
			\node [text width=2cm](s4) at (9,11) {Score for answer 4};
			\node [text width=2cm](s5) at (12,11) {Score for answer 5};
			
			\path [line] ([xshift=-6cm]integration.north) -- (s1);
			\path [line] ([xshift=-3cm]integration.north) -- (s2);
			\path [line] ([xshift=0cm]integration.north) -- (s3);
			\path [line] ([xshift=3cm]integration.north) -- (s4);
			\path [line] ([xshift=6cm]integration.north) -- (s5);
			
			%Leaving some room at the top of the graph
			\node at (6,12.5){};
			
			\end{tikzpicture}
		}
	\end{center}
	\caption{Our full model overview.}
	\label{fig:full_model_overview}
\end{figure}

\subsection{Dataset alterations}

In the original paper, each input sequence consisted of a classifier token ('[CLS]'), the question, a separator token ('[SEP]'), a possible answer, and then another separator token, as seen in figure \ref{fig:model_orig}. The classifier token is unique, as the final hidden state corresponding to that token was used as the aggregate sequence representation for the question-answering task.

\tikzset{
	font={\fontsize{12pt}{12}\selectfont}}


Our model deviates from these assumptions. Instead, each input sequence was constructed from the question and 2 possible answers in the following manner: A classifier token, the first answer, a separator token, the question, a separator token, the second answer, and another classifier token (As seen in figure \ref{fig:model}). Note, that this construction breaks several of the original BERT paper assumptions - each of our input sequences consist of more than just one classifier token, and more than just two sentences. As such, it is possible that BERT will suffer from reduced performance, as its entire training process was built around these assumptions.
However, breaking these assumptions was a crucial part of what made our constructions work - it allowed for each of the classifier tokens to "absorb" the meaning of the answer closer to it, and imporved the symmetry of our model in regards to each of the two answers.


\tikzset{
	font={\fontsize{13pt}{14}\selectfont}}

\begin{figure}[h!]
\begin{center}
	\scalebox{0.6}{
		\begin{tikzpicture}[node distance = 2cm, auto]
		\node [bigblock14] (bert) at (0,0) {Pretrained BERT};
		\node [block, below of=bert] (Question) {Question};
		\node [block, left of=Question] (Sep1) {[SEP]};
		\node [block, left of=Sep1] (Answer1) {Answer1};
		\node [block, left of=Answer1] (Cls1) {[CLS]};
		\node [block, right of=Question] (Sep2) {[SEP]};
		\node [block, right of=Sep2] (Answer2) {Answer2};
		\node [block, right of=Answer2] (Cls2) {[CLS]};
		\node [block] (classifier1) at (-6,2) {Classifier token 1};
		\node [block] (classifier2) at (6,2) {Classifier token 2};
		\node [block10] (rest) at (0,2) {Unused tokens};
		
		\path [line] (Cls1) -- ([xshift=-6cm]bert.south);
		\path [line] (Answer1) -- ([xshift=-4cm]bert.south);
		\path [line] (Sep1) -- ([xshift=-2cm]bert.south);
		\path [line] (Question) -- (bert.south);
		\path [line] (Sep2) -- ([xshift=2cm]bert.south);
		\path [line] (Answer2) -- ([xshift=4cm]bert.south);
		\path [line] (Cls2) -- ([xshift=6cm]bert.south);
		
		
		\path [line] ([xshift=-6cm]bert.north) -- (classifier1.south);
		\path [line] ([xshift=6cm]bert.north) -- (classifier2.south);
		
		\path [line,dashed] ([xshift=-4cm]bert.north) -- ([xshift=-4cm]rest.south);
		\path [line,dashed] ([xshift=-2cm]bert.north) -- ([xshift=-2cm]rest.south);
		\path [line,dashed] ([xshift=-0cm]bert.north) -- ([xshift=-0cm]rest.south);
		\path [line,dashed] ([xshift=2cm]bert.north) -- ([xshift=2cm]rest.south);
		\path [line,dashed] ([xshift=4cm]bert.north) -- ([xshift=4cm]rest.south);
		\end{tikzpicture}
	}
\end{center}
\caption{Our question-answering model.}
\label{fig:model}
\end{figure}

% $$
% P := \text{\textit{Learned parameter}}
% $$
% $$
% CT_{i,j} := \text{\textit{Classifier Tokens Concatened}}
% $$
% $$
% S'_{i,j} := P \cdot CT_{i,j}
% $$
% $$
% \text{\textit{Score}}_{i,j} := \frac{S'_{i,j} + (1 - S'_{j,i})}{2}
% $$

Another part of the input sequence given to BERT is the segment embeddings - a number from \{0, 1\} which signifies whether the current token is part of the first sentence or the second. After trial and error, it was found that we perform best when giving a value of 1 for \textbf{each} of the answer tokens, and 0 for the question tokens. That makes sense, as part of the desire for symmetry in our model, and the fact that BERT wasn't trained with a possibility for a third sentence, so using, for example, the number 2 for our second answer would not make any sense to it.

After passing our input sequence through BERT, we use a concatenation of the final hidden states matching the classifier tokens as the sequence representation. We learn as a parameter a vector whose dot product with the sequence representation is the score for the given answer pair (equation \ref{edge_score}). Then, we compute the edge score between answers i to j by averaging the sigmoid result of the two. 

% TODO: you wrote 'add a sentence', why? 


\begin{equation}
  \begin{aligned}
      \vec{CT_{i,j}} &:= \text{\textit{Classifier Tokens Concatenated \footnotemark}}\\ 
    \vec{P} &:= \text{\textit{Learned parameters}}\\
    S'_{i,j} &:= sigmoid(\vec{CT_{i,j}} \cdot \vec{P})\\
    \text{\textit{Score}}_{i,j} &:= \frac{S'_{i,j} + (1-S'_{j,i}))}{2}
  \end{aligned}
  \label{edge_score}
\end{equation}

\footnotetext{Classifier tokens concatenated, after running through the BERT model using answers $i, j$ for input}

\subsection{Pair scores integration}

After runnning the previously described part of the model, we receive a score for each of the answer pairs, meaning "How much more likely is answer A than answer B". We conceived a few methods of combining the different pair scores into probabilities for the answers. In this section we will review those methods and compare them.

\subsubsection{Learnt weights}

In this method, we introduce more layers to derive the indiviual probabilities from the pair scores. We use 2 layers, with a hidden size of $ (number\ of\ pairs)^2 $ and normalizing with a softmax layer. This generates a valid probability distribution over the possible answers.

\subsubsection{Markov chains}

In this method, we find the stationary distribution of a weighted graph whose edge's weights are derived from our pair scores.
The reasoning for this process is as follows - imagine a graph whose vertices are the given answers. We begin by choosing an answer randomly, and we want to move on to the best possible answer. At each time step, we move to a different answer, choosing based on the scores of the pairs in which our current answer participates, i.e. the better another answer performed against our current answer, the more likely we are to move to it.
This process is powerful, because it captures the dependency between the different answers, e.g. if answer B is better than answer A, and answer C is better than answer B, then we'd like to factor in that information when comparing between answers A and C. 

\tikzset{
	font={\fontsize{12pt}{12}\selectfont}}

\begin{figure}[h!]
	\begin{center}
		\scalebox{0.6}{
			\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
			\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
			\tikzstyle{every node}=[text=blue]
			\node[state]    (A)                     {$A$};
			\node[state]    (B)[above right of=A]   {$B$};
			\node[state]    (C)[below right of=A]   {$C$};
			\node[state]    (D)[below right of=B]   {$D$};
			\path
			(A) edge[loop left]     node{$0.3$}         (A)
			edge[bend left]     node{$0.2$}     (B)
			edge[bend left,below]      node[above=-0.1cm]{$0.45$}      (D)
			edge[bend right]    node[below=0.3cm]{$0.05$}      (C)
			(B) edge                node[above=-0.1cm]{$0.1$}           (D)
			edge[loop above]     node{$0.3$}         (B)
			edge                node[above=-0.1cm]{$0.05$}           (A)
			edge[bend right]    node[above=-0.1cm]{$0.55$}           (C)
			(C) edge                node[above=-0.1cm]{$0.7$}           (D)
			edge[bend right]    node[above=-0.1cm]{$0.1$}           (B)
			edge[loop below]     node{$0.05$}         (C)
			edge                node[above=-0.1cm]{$0.15$}           (A)
			(D) edge[loop right]    node{$0.55$}     (D)
			edge[bend right,right]    node{$0$}      (B)
			edge[bend left]     node{$0.2$}      (C)
			edge[bend left,above]     node{$0.25$}         (A);
			\end{tikzpicture}
		}
	\end{center}
	\caption{A markov chain with 4 nodes.}
	\label{fig:markov_chain}
\end{figure}

Simply constructing a matrix with each cell being the score for the matching pair, normalizing each row so that we have a probability matrix, and then calculating the stationary distribution doesn't work well in practice. This is because normalizing the matrix causes the model to be unable to capture the absolute value of the scores, e.g. if it sees a row in which all values are similar, it can't differ the case in which all scores were low (and thus our "current" answer is good) from the case all of the scores were high.
Consider the following pair-score matrix, extracted from a sample run of our previously discussed model:

$$
\begin{bmatrix}
0 & 0.95 & 0.85 & 0.25 \\
0.05 & 0 & 0.98 & 0.74 \\
0.15 & 0.02 & 0 & 0.45 \\
0.75 & 0.26 & 0.55 & 0
\end{bmatrix}
$$

We can see that the third answer had won in every comparison (every non-diagonal cell in the third column is $>0.5$) - thus, we would expect it to be assigned the highest probability. Given the currently described method, this in not the case, as the probabilities received would be:
$$ \begin{bmatrix} 0.22614831 & 0.16630443 & 0.29576885 & \textbf{0.31177841} \end{bmatrix} $$

We solve this problem by adding the average of each column to the matching cell on the diagonal. This makes sense, as this gives us the option of staying on the current choice, and the better a certain choice had performed against the other choices, the greater the possibility of staying on it.
After doing this, we derive this matrix for our example:

$$
\begin{bmatrix}
\textbf{0.31} & 0.95 & 0.85 & 0.25 \\
0.05 & \textbf{0.41} & 0.98 & 0.74 \\
0.15 & 0.02 & \textbf{0.79} & 0.45 \\
0.75 & 0.26 & 0.55 & \textbf{0.48}
\end{bmatrix}
$$

We then normalize the matrix and calculate the stationary probability distribution, which now seems to describe the relationships between our answers well:

$$ \begin{bmatrix} 0.16845236 & 0.13252954 & \textbf{0.43521662} & 0.26380148 \end{bmatrix} $$

\subsubsection{Tie-breakers}

This is not a method on its own, but it can be used on top of the other ones.
Essentially, we can assert a "Certainty Threshold" on the probabilities obtained from the other methods, a rate between two probabilities under which we consider our prediction to be uncertain/unstable.
If such a scenario arises, we then resort to choosing between the two highest-scored choices directly - that is, by looking at the score given to the pair and deciding accordingly.

Note that this change does not cause a change in the probabilities themselves, it only changes the predicted answer. As such, it does not cause a change in the measured loss, thus does not affect the learned parameters. It does change, however, the measured accuracy, which means that when learning parameters we will favor a parameter set that maximizes the accuracy given this change.

\section{Experiments and Results} 

As our model deviates greatly from the assumptions that were made when the original BERT weights were trained, it is understandable that our model required more epochs to converge than the amount of epochs the original paper used when fine-tuning to the question answering task.
We should also note that because each sample now consists of much more input sequences (the original model had one sequence per possible answer, our model has one sequence per possible answer \textbf{pair}), we are now more limited in our batch size, as the GPU has to hold the entire batch simultaneously. This problem can be mitigated by using gradient accumulation.

Our models were trained in parts.
In the first part, we trained solely on the dataset alterations part, without integrating the pair scores. That is, our model was trained directly on identifying the better answer from a pair of answers. After 5 epochs with a dynamic learning rate (starting from 1e-6 and decreasing to 4e-7) we were able to achieve a 70.3\% accuracy on the dev set. (Note that this accuracy is measured over pairs and not over questions - it is not comparable to the performance of other models over this dataset).

In the second part, we trained our model for 2 more epochs using each method of pair-scores integration suggested. Empirically, using the tie-breakers method on any of the suggested models caused a drop in performance, and thus it was discarded.
A comparison of our experiment results follows.

\begin{table}[h!]
	\label{tbl:accuracy}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lr}
					\toprule
					Method	& Accuracy (dev-set)\\
					\midrule
					Comparative Model + \\ \qquad \qquad Markov chains 	& 49.3\% \\
					Comparative Model + \\ \qquad \qquad Learnt weights 	& 49.1\% \\
					\midrule
					Random guess & 25\% \\
					Original BERT model (baseline) & \textbf{56.7\%} \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
	\caption{Full model comparison}
\end{table}

\section{Discussion}
\subsection{Train-Test Discrepancy}
When humans tackle multi-choice questions we compare all answer pairs simultaneously, disqualify some unreasonable answers and approve others before we make a prediction. Thus, we narrow the number of possible solutions in a multi-phase process. In this paper, this is exactly what we wanted to achieve when we compared all answers pairs altogether. One possible reason for our model's under-performance is Train Test Discrepancy; we are aiming to learn in a supervised way information which is not exactly given to us - In the training set, we know only the gold labels per each question and not the relationship between them. Therefore, there is no information about answers which are totally wrong or answers which are close to each other and can be the prediction candidates. 

Also, assuming we are given 5 possible answers, in the training phase we create 8 answer pairs - the gold answer with all the other answers (for each ordering of the pairs). However, at test time we create all possible answer pairs which are 20. It appears that at training time the model learns to discriminate between the right answer and the wrong answers, A dynamic which is not necessarily the same at test time when we feed all answer pairs together. This discrepancy can be inherent in the dataset and the way it was built, e.g. it may be that the gold answer is better then all the other candidate answer but some answers are distractors which "confuse" the model at test time.

\subsection{Conclusions}
Neither the Markovian model nor the Learnt weights model outperformed the baseline model. In fact, there was a siginficant gap between our methods and the baseline results. We believe that it is due to a combination of the following reasons:
\begin{itemize}
\item Train-test discrepancy issue.
\item BERT was trained in a different manner than our architecture. It is likely  that our input representation has harmed the information distillation from the original BERT model thus harming the final result. Which input configuration is optimal - is still in open question, in the sense that information from BERT model is distilled but also answer pair information is learnt. 
\item  Bigger datasets can help in this manner, by enabling us to generalize well input configurations that are different than BERT baseline.
\end{itemize}

\section{Future Work}
To solve the train-test discrepancy issue, we believe that it is possible to create a dataset with more detailed training labels, for example, ranking the answers and providing information about distractor answers. It is interesting to verify if we can achieve a better performance by removing this discrepancy.

Another possible approach to improve upon our work with a \textbf{ranked} dataset is to fully "neuralize" our Markov chain solution. For instance, one could create a model that feeds all the answer pairs jointly to  BERT, compute a logit for every pair and compare it with the \textbf{ranked} ground truth, then use convolutional layers to classify the correct answer. It can also be interesting to examine the graph layer of this network for explainability reasons.  

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \printbibliography
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
